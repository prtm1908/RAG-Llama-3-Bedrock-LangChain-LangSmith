{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prath\\anaconda3\\envs\\matr\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"]=\"\"\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = ''\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<botocore.client.Lambda object at 0x000001A6C9607290>\n"
     ]
    }
   ],
   "source": [
    "lambda_client = boto3.client(\n",
    "    'lambda',\n",
    "    region_name='us-east-1',\n",
    "    aws_access_key_id='',\n",
    "    aws_secret_access_key=''\n",
    ")\n",
    "\n",
    "print(lambda_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prath\\anaconda3\\envs\\matr\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:12<00:00, 18.20s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(model_checkpoint,\n",
    "                                        trust_remote_code=True,\n",
    "                                        max_new_tokens=1024)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint,\n",
    "                                            trust_remote_code=True,\n",
    "                                            config=model_config,\n",
    "                                            device_map='auto')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "pipeline = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                torch_dtype=torch.float16,\n",
    "                max_length=3000,\n",
    "                device_map=\"auto\",)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(question: str):\n",
    "    # Invoke the Lambda function\n",
    "    response = lambda_client.invoke(\n",
    "        FunctionName='prtmraginference',\n",
    "        InvocationType='RequestResponse',\n",
    "        Payload=json.dumps({\"question\": question})\n",
    "    )\n",
    "\n",
    "    print(response)\n",
    "    \n",
    "    # Read the Lambda function's response stream and parse it\n",
    "    response_payload = response['Payload'].read()\n",
    "    response_payload_dict = json.loads(response_payload)\n",
    "    \n",
    "    # Navigate to the retrievalResults\n",
    "    results = response_payload_dict['body']['answer']['retrievalResults']\n",
    "\n",
    "    print(results)\n",
    "    \n",
    "    # Initialize an empty string to store the extracted paragraph\n",
    "    extracted_paragraph = \"\"\n",
    "    \n",
    "    # Loop through each result and concatenate text to a paragraph\n",
    "    for result in results:\n",
    "        text = result['content']['text']\n",
    "        extracted_paragraph += text + \" \"\n",
    "\n",
    "    # Return the concatenated paragraph\n",
    "    return {\"response\": extracted_paragraph.strip()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_kb(query: str, context:str, llm):\n",
    "    kb_prompt_template = f\"\"\"<|begin_of_text|>\n",
    "<|start_header_id|>\n",
    "    system\n",
    "<|end_header_id|>\n",
    "    You are a helpful, respectful and honest assistant designated answer questions related to the user's document.If the user tries to ask out of topic questions do not engange in the conversation.If the given context is not sufficient to answer the question,Do not answer the question.\n",
    "<|eot_id|>\n",
    "<|start_header_id|>\n",
    "    user\n",
    "<|end_header_id|>\n",
    "    Answer the user question based on the context provided below\n",
    "    Context :{context}\n",
    "    Question: {query}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>\n",
    "    assistant\n",
    "<|end_header_id|>\"\"\"\n",
    "\n",
    "    prompt_template_kb = PromptTemplate(\n",
    "        input_variables=[\"context\", \"query\"], template=kb_prompt_template\n",
    "    )\n",
    "\n",
    "    llm_chain = LLMChain(llm=llm, prompt=prompt_template_kb)\n",
    "    \n",
    "    result = llm_chain.run({\"context\":context, \"query\":query})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Compare 2B and 7B models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': 'd6150ae8-61c6-453a-bedd-415f30f25f6c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sun, 05 May 2024 13:13:33 GMT', 'content-type': 'application/json', 'content-length': '5742', 'connection': 'keep-alive', 'x-amzn-requestid': 'd6150ae8-61c6-453a-bedd-415f30f25f6c', 'x-amzn-remapped-content-length': '0', 'x-amz-executed-version': '$LATEST', 'x-amzn-trace-id': 'root=1-663785fb-5bbc888d3d843ee326f1b4a7;parent=11646bcaf148b9e2;sampled=0;lineage=1c3db0ec:0'}, 'RetryAttempts': 0}, 'StatusCode': 200, 'ExecutedVersion': '$LATEST', 'Payload': <botocore.response.StreamingBody object at 0x000001A6C9D13100>}\n",
      "[{'content': {'text': 'We also utilize several improvements proposed after the original trans-   Parameters 2B 7B   d_model 2048 3072 Layers 18 28 Feedforward hidden dims 32768 49152 Num heads 8 16 Num KV heads 1 16 Head size 256 256 Vocab size 256128 256128   Table 1 | Key model parameters.   former paper, and list them below: Multi-Query Attention (Shazeer, 2019). No- tably, the 7B model uses multi-head attention while the 2B checkpoints use multi-query atten- tion (with ð�‘›ð�‘¢ð�‘š_ð�‘˜ð�‘£_â„Žð�‘’ð�‘Žð�‘‘ð�‘\\xa0 = 1), based on ablations that showed that multi-query attention works well at small scales (Shazeer, 2019). RoPE Embeddings (Su et al., 2021). Rather than using absolute positional embeddings, we use ro- tary positional embeddings in each layer; we also share embeddings across our inputs and outputs to reduce model size. GeGLU Activations (Shazeer, 2020).'}, 'location': {'type': 'S3', 's3Location': {'uri': 's3://prtmrag/dataset/GemmaPDF.pdf'}}, 'score': 0.43464962}, {'content': {'text': 'Side-by-side evaluations of Gemma 1.0 IT against Mistral 7b v0.2 can be found in table 9. Safety academic benchmark results of version 1.0 can be found in table 10.   Model Safety Instruction Following Gemma 7B IT 58% 51.7% 95% Conf. Interval [55.9%, 60.1%] [49.6%, 53.8%] Win / Tie / Loss 42.9% / 30.2% / 26.9% 42.5% / 18.4% / 39.1%   Gemma 2B IT 56.5% 41.6% 95% Conf. Interval [54.4%, 58.6%] [39.5%, 43.7%] Win / Tie / Loss 44.8% / 22.9% / 32.3% 32.7% / 17.8% / 49.5%   Table 9 | Win rate of Gemma 1.0 IT models versus Mistral 7B v0.2 Instruct with 95% confidence intervals. We report breakdowns of wins, ties, and losses. Ties are broken evenly in the final win rate.'}, 'location': {'type': 'S3', 's3Location': {'uri': 's3://prtmrag/dataset/GemmaPDF.pdf'}}, 'score': 0.43200645}, {'content': {'text': '22.0 32.3 MBPPâ€\\xa0 3-shot 20.8 30.6 40.2âˆ— 29.2 44.4 GSM8K maj@1 14.6 28.7 35.4âˆ— 17.7 46.4 MATH 4-shot 2.5 3.9 12.7 11.8 24.3   AGIEval 29.3 39.1 41.2âˆ— 24.2 41.7 BBH 32.6 39.4 56.1âˆ— 35.2 55.1 Average 46.9 52.4 54.5 45.0 56.9   Table 6 | Academic benchmark results, compared to similarly sized, openly-available models trained on general English text data. â€\\xa0 Mistral reports 50.2 on a different split for MBPP and on their split our 7B model achieves 54.5. âˆ— evaluations run by us. Note that due to restrictive licensing, we were unable to run evals on LLaMA-2; all values above were previously reported in Touvron et al. (2023b).   outperform other models by at least 10 points on GSM8K (Cobbe et al., 2021) and the more difficult MATH (Hendrycks et al., 2021) bench- mark. Similarly, they outperform alternate open models by at least 6 points on HumanEval (Chen et al., 2021).'}, 'location': {'type': 'S3', 's3Location': {'uri': 's3://prtmrag/dataset/GemmaPDF.pdf'}}, 'score': 0.4244675}, {'content': {'text': 'Note that due to restrictive licensing, we were unable to run evals on LLaMA-2; we do not report previously-published numbers for LLaMA-2 on TruthfulQA, as we use different, non-comparable evaluation set-ups: we use MC2, where LLaMA-2 uses GPT-Judge. Results for Gemma 1.0 IT models can be found in appendix.   Code Wiki Science Web   Multilin gual   Data Source   0.1   1   10   %  M   em or   ize d   2B Model   Code Wiki Science Web   Multilin gual   Data Source   0.1   1   10   %  M   em or   ize d   7B Model   Memorization Type Exact Approximate   Figure 4 | Comparing exact and approximate memorization.   mately memorized (note the log scale) and that this is nearly consistent across each of the differ- ent subcategories over the dataset.'}, 'location': {'type': 'S3', 's3Location': {'uri': 's3://prtmrag/dataset/GemmaPDF.pdf'}}, 'score': 0.41821873}, {'content': {'text': 'and on their split our 7B model achieves 54.5. âˆ— evaluations run by us. Note that due to restrictive licensing, we were unable to run evals on LLaMA-2; all values above were previously reported in Touvron et al. (2023b).   outperform other models by at least 10 points on GSM8K (Cobbe et al., 2021) and the more difficult MATH (Hendrycks et al., 2021) bench- mark. Similarly, they outperform alternate open models by at least 6 points on HumanEval (Chen et al., 2021). They even surpass the performance of the code-fine-tuned CodeLLaMA-7B models on MBPP (CodeLLaMA achieves a score of 41.4% where Gemma 7B achieves 44.4%).   Memorization Evaluations   Recent work has shown that aligned models may be vulnerable to new adversarial attacks that can bypass alignment (Nasr et al., 2023). These at- tacks can cause models to diverge, and sometimes regurgitate memorized training data in the pro- cess. We focus on discoverable memorization, which serves as a reasonable upper-bound on the   memorization of a model (Nasr et al., 2023) and has been the common definition used in several studies (Anil et al., 2023; Carlini et al., 2022; Kudugunta et al., 2023).'}, 'location': {'type': 'S3', 's3Location': {'uri': 's3://prtmrag/dataset/GemmaPDF.pdf'}}, 'score': 0.41155577}]\n",
      "We also utilize several improvements proposed after the original trans-   Parameters 2B 7B   d_model 2048 3072 Layers 18 28 Feedforward hidden dims 32768 49152 Num heads 8 16 Num KV heads 1 16 Head size 256 256 Vocab size 256128 256128   Table 1 | Key model parameters.   former paper, and list them below: Multi-Query Attention (Shazeer, 2019). No- tably, the 7B model uses multi-head attention while the 2B checkpoints use multi-query atten- tion (with ð�‘›ð�‘¢ð�‘š_ð�‘˜ð�‘£_â„Žð�‘’ð�‘Žð�‘‘ð�‘  = 1), based on ablations that showed that multi-query attention works well at small scales (Shazeer, 2019). RoPE Embeddings (Su et al., 2021). Rather than using absolute positional embeddings, we use ro- tary positional embeddings in each layer; we also share embeddings across our inputs and outputs to reduce model size. GeGLU Activations (Shazeer, 2020). Side-by-side evaluations of Gemma 1.0 IT against Mistral 7b v0.2 can be found in table 9. Safety academic benchmark results of version 1.0 can be found in table 10.   Model Safety Instruction Following Gemma 7B IT 58% 51.7% 95% Conf. Interval [55.9%, 60.1%] [49.6%, 53.8%] Win / Tie / Loss 42.9% / 30.2% / 26.9% 42.5% / 18.4% / 39.1%   Gemma 2B IT 56.5% 41.6% 95% Conf. Interval [54.4%, 58.6%] [39.5%, 43.7%] Win / Tie / Loss 44.8% / 22.9% / 32.3% 32.7% / 17.8% / 49.5%   Table 9 | Win rate of Gemma 1.0 IT models versus Mistral 7B v0.2 Instruct with 95% confidence intervals. We report breakdowns of wins, ties, and losses. Ties are broken evenly in the final win rate. 22.0 32.3 MBPPâ€  3-shot 20.8 30.6 40.2âˆ— 29.2 44.4 GSM8K maj@1 14.6 28.7 35.4âˆ— 17.7 46.4 MATH 4-shot 2.5 3.9 12.7 11.8 24.3   AGIEval 29.3 39.1 41.2âˆ— 24.2 41.7 BBH 32.6 39.4 56.1âˆ— 35.2 55.1 Average 46.9 52.4 54.5 45.0 56.9   Table 6 | Academic benchmark results, compared to similarly sized, openly-available models trained on general English text data. â€  Mistral reports 50.2 on a different split for MBPP and on their split our 7B model achieves 54.5. âˆ— evaluations run by us. Note that due to restrictive licensing, we were unable to run evals on LLaMA-2; all values above were previously reported in Touvron et al. (2023b).   outperform other models by at least 10 points on GSM8K (Cobbe et al., 2021) and the more difficult MATH (Hendrycks et al., 2021) bench- mark. Similarly, they outperform alternate open models by at least 6 points on HumanEval (Chen et al., 2021). Note that due to restrictive licensing, we were unable to run evals on LLaMA-2; we do not report previously-published numbers for LLaMA-2 on TruthfulQA, as we use different, non-comparable evaluation set-ups: we use MC2, where LLaMA-2 uses GPT-Judge. Results for Gemma 1.0 IT models can be found in appendix.   Code Wiki Science Web   Multilin gual   Data Source   0.1   1   10   %  M   em or   ize d   2B Model   Code Wiki Science Web   Multilin gual   Data Source   0.1   1   10   %  M   em or   ize d   7B Model   Memorization Type Exact Approximate   Figure 4 | Comparing exact and approximate memorization.   mately memorized (note the log scale) and that this is nearly consistent across each of the differ- ent subcategories over the dataset. and on their split our 7B model achieves 54.5. âˆ— evaluations run by us. Note that due to restrictive licensing, we were unable to run evals on LLaMA-2; all values above were previously reported in Touvron et al. (2023b).   outperform other models by at least 10 points on GSM8K (Cobbe et al., 2021) and the more difficult MATH (Hendrycks et al., 2021) bench- mark. Similarly, they outperform alternate open models by at least 6 points on HumanEval (Chen et al., 2021). They even surpass the performance of the code-fine-tuned CodeLLaMA-7B models on MBPP (CodeLLaMA achieves a score of 41.4% where Gemma 7B achieves 44.4%).   Memorization Evaluations   Recent work has shown that aligned models may be vulnerable to new adversarial attacks that can bypass alignment (Nasr et al., 2023). These at- tacks can cause models to diverge, and sometimes regurgitate memorized training data in the pro- cess. We focus on discoverable memorization, which serves as a reasonable upper-bound on the   memorization of a model (Nasr et al., 2023) and has been the common definition used in several studies (Anil et al., 2023; Carlini et al., 2022; Kudugunta et al., 2023).\n"
     ]
    }
   ],
   "source": [
    "context=get_context(query)\n",
    "context=context['response']\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prath\\anaconda3\\envs\\matr\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\prath\\anaconda3\\envs\\matr\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': 'c400e939-d512-4e36-a9e8-56551f4a1f0f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sun, 05 May 2024 13:13:34 GMT', 'content-type': 'application/json', 'content-length': '5742', 'connection': 'keep-alive', 'x-amzn-requestid': 'c400e939-d512-4e36-a9e8-56551f4a1f0f', 'x-amzn-remapped-content-length': '0', 'x-amz-executed-version': '$LATEST', 'x-amzn-trace-id': 'root=1-663785fd-653923c83a6f596a319f194e;parent=4a69eab2f43f8adb;sampled=0;lineage=1c3db0ec:0'}, 'RetryAttempts': 0}, 'StatusCode': 200, 'ExecutedVersion': '$LATEST', 'Payload': <botocore.response.StreamingBody object at 0x000001A6C9D12E90>}\n",
      "[{'content': {'text': 'We also utilize several improvements proposed after the original trans-   Parameters 2B 7B   d_model 2048 3072 Layers 18 28 Feedforward hidden dims 32768 49152 Num heads 8 16 Num KV heads 1 16 Head size 256 256 Vocab size 256128 256128   Table 1 | Key model parameters.   former paper, and list them below: Multi-Query Attention (Shazeer, 2019). No- tably, the 7B model uses multi-head attention while the 2B checkpoints use multi-query atten- tion (with ð�‘›ð�‘¢ð�‘š_ð�‘˜ð�‘£_â„Žð�‘’ð�‘Žð�‘‘ð�‘\\xa0 = 1), based on ablations that showed that multi-query attention works well at small scales (Shazeer, 2019). RoPE Embeddings (Su et al., 2021). Rather than using absolute positional embeddings, we use ro- tary positional embeddings in each layer; we also share embeddings across our inputs and outputs to reduce model size. GeGLU Activations (Shazeer, 2020).'}, 'location': {'type': 'S3', 's3Location': {'uri': 's3://prtmrag/dataset/GemmaPDF.pdf'}}, 'score': 0.43464962}, {'content': {'text': 'Side-by-side evaluations of Gemma 1.0 IT against Mistral 7b v0.2 can be found in table 9. Safety academic benchmark results of version 1.0 can be found in table 10.   Model Safety Instruction Following Gemma 7B IT 58% 51.7% 95% Conf. Interval [55.9%, 60.1%] [49.6%, 53.8%] Win / Tie / Loss 42.9% / 30.2% / 26.9% 42.5% / 18.4% / 39.1%   Gemma 2B IT 56.5% 41.6% 95% Conf. Interval [54.4%, 58.6%] [39.5%, 43.7%] Win / Tie / Loss 44.8% / 22.9% / 32.3% 32.7% / 17.8% / 49.5%   Table 9 | Win rate of Gemma 1.0 IT models versus Mistral 7B v0.2 Instruct with 95% confidence intervals. We report breakdowns of wins, ties, and losses. Ties are broken evenly in the final win rate.'}, 'location': {'type': 'S3', 's3Location': {'uri': 's3://prtmrag/dataset/GemmaPDF.pdf'}}, 'score': 0.43200645}, {'content': {'text': '22.0 32.3 MBPPâ€\\xa0 3-shot 20.8 30.6 40.2âˆ— 29.2 44.4 GSM8K maj@1 14.6 28.7 35.4âˆ— 17.7 46.4 MATH 4-shot 2.5 3.9 12.7 11.8 24.3   AGIEval 29.3 39.1 41.2âˆ— 24.2 41.7 BBH 32.6 39.4 56.1âˆ— 35.2 55.1 Average 46.9 52.4 54.5 45.0 56.9   Table 6 | Academic benchmark results, compared to similarly sized, openly-available models trained on general English text data. â€\\xa0 Mistral reports 50.2 on a different split for MBPP and on their split our 7B model achieves 54.5. âˆ— evaluations run by us. Note that due to restrictive licensing, we were unable to run evals on LLaMA-2; all values above were previously reported in Touvron et al. (2023b).   outperform other models by at least 10 points on GSM8K (Cobbe et al., 2021) and the more difficult MATH (Hendrycks et al., 2021) bench- mark. Similarly, they outperform alternate open models by at least 6 points on HumanEval (Chen et al., 2021).'}, 'location': {'type': 'S3', 's3Location': {'uri': 's3://prtmrag/dataset/GemmaPDF.pdf'}}, 'score': 0.4244675}, {'content': {'text': 'Note that due to restrictive licensing, we were unable to run evals on LLaMA-2; we do not report previously-published numbers for LLaMA-2 on TruthfulQA, as we use different, non-comparable evaluation set-ups: we use MC2, where LLaMA-2 uses GPT-Judge. Results for Gemma 1.0 IT models can be found in appendix.   Code Wiki Science Web   Multilin gual   Data Source   0.1   1   10   %  M   em or   ize d   2B Model   Code Wiki Science Web   Multilin gual   Data Source   0.1   1   10   %  M   em or   ize d   7B Model   Memorization Type Exact Approximate   Figure 4 | Comparing exact and approximate memorization.   mately memorized (note the log scale) and that this is nearly consistent across each of the differ- ent subcategories over the dataset.'}, 'location': {'type': 'S3', 's3Location': {'uri': 's3://prtmrag/dataset/GemmaPDF.pdf'}}, 'score': 0.41821873}, {'content': {'text': 'and on their split our 7B model achieves 54.5. âˆ— evaluations run by us. Note that due to restrictive licensing, we were unable to run evals on LLaMA-2; all values above were previously reported in Touvron et al. (2023b).   outperform other models by at least 10 points on GSM8K (Cobbe et al., 2021) and the more difficult MATH (Hendrycks et al., 2021) bench- mark. Similarly, they outperform alternate open models by at least 6 points on HumanEval (Chen et al., 2021). They even surpass the performance of the code-fine-tuned CodeLLaMA-7B models on MBPP (CodeLLaMA achieves a score of 41.4% where Gemma 7B achieves 44.4%).   Memorization Evaluations   Recent work has shown that aligned models may be vulnerable to new adversarial attacks that can bypass alignment (Nasr et al., 2023). These at- tacks can cause models to diverge, and sometimes regurgitate memorized training data in the pro- cess. We focus on discoverable memorization, which serves as a reasonable upper-bound on the   memorization of a model (Nasr et al., 2023) and has been the common definition used in several studies (Anil et al., 2023; Carlini et al., 2022; Kudugunta et al., 2023).'}, 'location': {'type': 'S3', 's3Location': {'uri': 's3://prtmrag/dataset/GemmaPDF.pdf'}}, 'score': 0.41155577}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prath\\anaconda3\\envs\\matr\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:671: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>\n",
      "<|start_header_id|>\n",
      "system\n",
      "<|end_header_id|>\n",
      "You are a helpful, respectful and honest assistant designated answer questions related to the user's document.If the user tries to ask out of topic questions do not engange in the conversation.If the given context is not sufficient to answer the question,Do not answer the question.\n",
      "<|eot_id|>\n",
      "<|start_header_id|>\n",
      "user\n",
      "<|end_header_id|>\n",
      "Answer the user question based on the context provided below\n",
      "Context :We also utilize several improvements proposed after the original trans-   Parameters 2B 7B   d_model 2048 3072 Layers 18 28 Feedforward hidden dims 32768 49152 Num heads 8 16 Num KV heads 1 16 Head size 256 256 Vocab size 256128 256128   Table 1 | Key model parameters.   former paper, and list them below: Multi-Query Attention (Shazeer, 2019). No- tably, the 7B model uses multi-head attention while the 2B checkpoints use multi-query atten- tion (with ð�‘›ð�‘¢ð�‘š_ð�‘˜ð�‘£_â„Žð�‘’ð�‘Žð�‘‘ð�‘  = 1), based on ablations that showed that multi-query attention works well at small scales (Shazeer, 2019). RoPE Embeddings (Su et al., 2021). Rather than using absolute positional embeddings, we use ro- tary positional embeddings in each layer; we also share embeddings across our inputs and outputs to reduce model size. GeGLU Activations (Shazeer, 2020). Side-by-side evaluations of Gemma 1.0 IT against Mistral 7b v0.2 can be found in table 9. Safety academic benchmark results of version 1.0 can be found in table 10.   Model Safety Instruction Following Gemma 7B IT 58% 51.7% 95% Conf. Interval [55.9%, 60.1%] [49.6%, 53.8%] Win / Tie / Loss 42.9% / 30.2% / 26.9% 42.5% / 18.4% / 39.1%   Gemma 2B IT 56.5% 41.6% 95% Conf. Interval [54.4%, 58.6%] [39.5%, 43.7%] Win / Tie / Loss 44.8% / 22.9% / 32.3% 32.7% / 17.8% / 49.5%   Table 9 | Win rate of Gemma 1.0 IT models versus Mistral 7B v0.2 Instruct with 95% confidence intervals. We report breakdowns of wins, ties, and losses. Ties are broken evenly in the final win rate. 22.0 32.3 MBPPâ€  3-shot 20.8 30.6 40.2âˆ— 29.2 44.4 GSM8K maj@1 14.6 28.7 35.4âˆ— 17.7 46.4 MATH 4-shot 2.5 3.9 12.7 11.8 24.3   AGIEval 29.3 39.1 41.2âˆ— 24.2 41.7 BBH 32.6 39.4 56.1âˆ— 35.2 55.1 Average 46.9 52.4 54.5 45.0 56.9   Table 6 | Academic benchmark results, compared to similarly sized, openly-available models trained on general English text data. â€  Mistral reports 50.2 on a different split for MBPP and on their split our 7B model achieves 54.5. âˆ— evaluations run by us. Note that due to restrictive licensing, we were unable to run evals on LLaMA-2; all values above were previously reported in Touvron et al. (2023b).   outperform other models by at least 10 points on GSM8K (Cobbe et al., 2021) and the more difficult MATH (Hendrycks et al., 2021) bench- mark. Similarly, they outperform alternate open models by at least 6 points on HumanEval (Chen et al., 2021). Note that due to restrictive licensing, we were unable to run evals on LLaMA-2; we do not report previously-published numbers for LLaMA-2 on TruthfulQA, as we use different, non-comparable evaluation set-ups: we use MC2, where LLaMA-2 uses GPT-Judge. Results for Gemma 1.0 IT models can be found in appendix.   Code Wiki Science Web   Multilin gual   Data Source   0.1   1   10   %  M   em or   ize d   2B Model   Code Wiki Science Web   Multilin gual   Data Source   0.1   1   10   %  M   em or   ize d   7B Model   Memorization Type Exact Approximate   Figure 4 | Comparing exact and approximate memorization.   mately memorized (note the log scale) and that this is nearly consistent across each of the differ- ent subcategories over the dataset. and on their split our 7B model achieves 54.5. âˆ— evaluations run by us. Note that due to restrictive licensing, we were unable to run evals on LLaMA-2; all values above were previously reported in Touvron et al. (2023b).   outperform other models by at least 10 points on GSM8K (Cobbe et al., 2021) and the more difficult MATH (Hendrycks et al., 2021) bench- mark. Similarly, they outperform alternate open models by at least 6 points on HumanEval (Chen et al., 2021). They even surpass the performance of the code-fine-tuned CodeLLaMA-7B models on MBPP (CodeLLaMA achieves a score of 41.4% where Gemma 7B achieves 44.4%).   Memorization Evaluations   Recent work has shown that aligned models may be vulnerable to new adversarial attacks that can bypass alignment (Nasr et al., 2023). These at- tacks can cause models to diverge, and sometimes regurgitate memorized training data in the pro- cess. We focus on discoverable memorization, which serves as a reasonable upper-bound on the   memorization of a model (Nasr et al., 2023) and has been the common definition used in several studies (Anil et al., 2023; Carlini et al., 2022; Kudugunta et al., 2023).\n",
      "Question: Compare 2B and 7B models\n",
      "<|eot_id|>\n",
      "<|start_header_id|>\n",
      "assistant\n",
      "<|end_header_id|>\n",
      "Based on the provided context, here are the comparisons between the 2B and 7B models:\n",
      "\n",
      "* The 7B model uses multi-head attention, while the 2B model uses multi-query attention.\n",
      "* The 7B model outperforms the 2B model on the GSM8K benchmark, achieving a score of 44.4% compared to the 2B model's score of 32.3%.\n",
      "* The 7B model also outperforms the 2B model on the MATH benchmark, achieving a score of 56.1% compared to the 2B model's score of 41.2%.\n",
      "* The 7B model has a higher win rate than the 2B model in the Safety academic benchmark, with a win rate of 42.5% compared to the 2B model's win rate of 32.7%.\n",
      "* The 7B model has a higher average score than the 2B model in the academic benchmark, with an average score of 56.9% compared to the 2B model's average score of 45.0%.\n",
      "\n",
      "Overall, the 7B model appears to outperform the 2B model in most of the comparisons made in the provided context.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer_from_kb(query, context, pipeline))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
